Reducing Deep Network Complexity with Fourier Transform Methods

    intro: Harvard University
    arxiv: https://arxiv.org/abs/1801.01451
    github: https://github.com/andrew-jeremy/Reducing-Deep-Network-Complexity-with-Fourier-Transform-Method
    
    Main:uses shallow densely connected neuron network architectures
    通过对输入进行傅立叶变换，训练样本中的每个点都具有来自于所有其他点的所有加权信息的表示能量。
    （对输入进行变换，而非卷积核）。将输入图片看成2D正交奇函数的加权，并使用完全链接的单层网络来替代输出（训练和推测类别），不再需要conv和pool层
    降低神经网络的复杂度，减少计算负担并提高对大量训练样例的要求以实现高分类的精度（>98%）
    //需要大量存储空间来存储图像傅立叶变换的结果。基本上相当于一种新的方法，和CNN关系不那么大。
    Test envirionment: benchmark
    Test datasets: MNIST, CIFAR10, CIFAR100
    Test model： 傅立叶变换所有的训练和测试数据集，并使用一个完全连接的双层密集神经元网络模型和一个隐藏单元

Domain-adaptive deep network compression

    intro: ICCV 2017
    arxiv: https://arxiv.org/abs/1709.01041
    github: https://github.com/mmasana/DALR
    
    Main:
    在大数据集上训练的深度神经网络可以通过称为微调（fine-tuning）的过程，轻松的转移到新的域中，而标记实例的数量会少得多。
    //预训练的模型可以用在识别更少的标签数量的数据集当中。
    域名转换会导致网络激活的巨大转变，并且在压缩时需要考虑到这一点。证明考虑activation statistics（激活统计）时，压缩权重会导致封闭形式的解决方式的秩约束回归问题
    使用本文中提出的DALR（Domain Adaptive Low Rank (DALR)，可以更好的消除权重中的冗余。
    //考虑在将预训练模型转移到新的较小的类别的数据集中时，使用一种“域自适应低阶压缩”技术（低阶逼近技术），考虑到转移到另一个域时所发生的激活统计的转变。
    Test results：
    the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone
    增加5%-20%的压缩程度，在几乎无损精度的情况下。
    
ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks

    arxiv: https://arxiv.org/abs/1706.02393
    github: https://github.com/gudovskiy/ShiftCNN
    
    Main:(低精度近似，未认真读）
    ShiftCNN基于两权的权重表示，因此仅执行移位和加法操作。此外，ShiftCNN通过预先计算卷积项大大降低了卷积层的计算成本。
    可用于具有相对较小的权重的任何CNN架构，并允许至少降低两个数量级。不需要重训练，可以直接进行转化且错误率控制在1%。

Ultimate tensorization: compressing convolutional and FC layers alike

    intro: NIPS 2016 workshop: Learning with Tensors: Why Now and How?
    arxiv: https://arxiv.org/abs/1611.03214
    github: https://github.com/timgaripov/TensorNet-TF
    
    Main:
    对卷积核进行分解。
    压缩包括卷积层和全连接层，达到80x的压缩比，而精度损失1.1%
